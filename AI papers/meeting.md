## 用AI学AI

### 沉浸式翻译

doubao 1.5 lite                                      Claude 4 (用Artifact做可视化资料)

DeepSeek(火山)                                    Gemini 2.5 (用好搜索/DeepResearch)

GLN-4-flsah (开箱即用)                       DeepSeek R1+搜索 (问题的详细解释)

## 视频易上手书籍成体系

### 视频教程：

吴恩达的机器学习/AI课程

李宏毅：生成式AI时代下的机器学习（2025）

B站：李沐论文精读系列

B站：3Blue1Brown的数学与神经网络介绍部分

B站：王木头学科学

B站：Zomi

### 书籍

《深度学习入门：基于Python的理论与实现》

《the document is all your need》

《动手学深度学习（PyTorch版》

《深度学习的数学》

### 小建议

80%内容在英文内容上

## 基础设施的演进

**Brook-> COTS-> The Better Lession-> Scaling Law->** 

### Scaling Law，上帝的指挥棒（2020.01）

通常指的是在深度学习领域发现的“规模定律”，即随着模型参数数量、数据量和计算量的增加，模型性能会以某种规律性提升。这一规律被称为“上帝的指挥棒”，意味着只要不断扩大模型和数据规模，模型的表现就会持续提升。这一发现最早在 2020 年初被 OpenAI 等团队系统性提出，对 AI 研究和工业界产生了深远影响。

简而言之，Scaling Law 告诉我们：在一定范围内，模型越大、数据越多，AI 的能力就越强。这就像“上帝的指挥棒”一样，为 AI 发展指明了方向。

### The Hardware Lottery 当我们回顾GPU时代（2020.09）

“The Hardware Lottery” 是 AI 领域著名的观点，最早由 Sara Hooker 在 2020 年 9 月提出。它指出，AI 研究和深度学习的发展，常常受到硬件条件的极大影响。所谓“硬件彩票”，指的是某些算法或模型之所以流行和成功，并不完全因为它们理论上最优，而是因为它们恰好适配了当时主流的硬件（如 GPU）。

回顾 GPU 时代，深度学习的爆发与 GPU 的并行计算能力密不可分。许多神经网络架构（如卷积神经网络 CNN、Transformer 等）能够高效运行，正是因为 GPU 擅长大规模矩阵运算。如果当时主流硬件不是 GPU，而是其他类型的加速器，AI 领域可能会流行完全不同的算法和模型。

DeepSeek硬件彩票，比Open AI用更少的显卡，更少的数据

因此，“The Hardware Lottery” 提醒我们，AI 研究的方向和成果，往往受到硬件发展的“指挥棒”影响。理解这一点，有助于我们更全面地看待技术选择和创新的偶然性。

### LAION-5B，开源社区的英雄主义（2022.10）

LAION-5B 是 2022 年开源社区发布的超大规模多模态数据集，包含约 50 亿对图像与文本描述。它的出现极大推动了 AI 领域，尤其是大模型（如 CLIP、Stable Diffusion 等）的训练和创新。

“开源社区的英雄主义”体现在以下几个方面：

1. **资源共享** ：LAION-5B 由全球志愿者和研究者共同构建，免费开放给所有人使用，降低了 AI 研究的门槛。
2. **推动创新** ：有了这样的大规模数据集，更多团队和个人可以训练自己的多模态模型，促进了 AI 技术的多样化和快速发展。
3. **协作精神** ：项目汇聚了全球开源社区的力量，展现了协作、无私和开放的精神。
4. **影响深远** ：LAION-5B 成为许多知名 AI 模型的基础数据集，推动了生成式 AI、图文检索等多个方向的突破。

## 模型结构的演进

AlexNet-> Attention-> distlling-> ResNet-> Transformer-> PPO-> XLNet-> RoFormer-> LoRA-> CoT

### AlexNet，深度学习的开端（2012.10）

  **ImageNet Classification with Deep Convolutional Neural Networks**

  作者：Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton

AlexNet 是 2012 年 10 月由 Alex Krizhevsky 等人提出的深度卷积神经网络模型。它在 ImageNet 图像识别竞赛中以巨大优势夺冠，极大推动了深度学习在计算机视觉领域的发展，被认为是现代深度学习浪潮的开端。

AlexNet 的创新主要体现在以下几个方面：

1. **深层结构** ：采用了 8 层神经网络，包括6250万参数（5 层卷积层 + 3 层全连接层），远超当时主流模型的深度。
2. **ReLU 激活函数** ：首次大规模使用 ReLU替代传统神经元，有效缓解了梯度消失问题，加快了训练速度。
3. **GPU 加速** ：利用 GPU 进行大规模并行计算，使得训练大模型成为可能。
4. **数据增强与 Dropout** ：通过数据增强和 Dropout 技术，有效提升了模型的泛化能力，减少了过拟合。

AlexNet 的成功标志着深度学习时代的到来，开启了神经网络在图像、语音、自然语言处理等领域的广泛应用。
